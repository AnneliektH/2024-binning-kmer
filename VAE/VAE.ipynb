{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c7bf6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Concatenate, Lambda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.layers import Layer\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fa7e72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !mamba install --yes --prefix {sys.prefix} matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc565b-09c8-4596-ad16-d59b06995781",
   "metadata": {},
   "source": [
    "# To train the VAE in the provided code, you'll follow these steps:\n",
    "\n",
    "1. Prepare your data: Concatenate the metagenomic sequences, abundances, and trinucleotide frequencies into a single DataFrame.\n",
    "2. Define the VAE architecture: Use the build_vae function to create the VAE model.\n",
    "3. Preprocess the data: Standardize your data to have zero mean and unit variance, which helps the model converge faster.\n",
    "4. Train the VAE: Fit the VAE model to your preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae3c0a-66cd-49ff-b23f-d4e292c90296",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "- load dfs\n",
    "- concatenate on contig name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "831ed7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 3 mer frequencies\n",
    "df_3mer = pd.read_csv('ERR3211994_3mer.csv')\n",
    "\n",
    "\n",
    "# Load the metag k15 frequencies\n",
    "df_mgabund = pd.read_csv('all_mgsearch.abund.csv')\n",
    "df_mgabund['read_name'] = df_mgabund['query_filename'].str.replace(r'fasta/', '', regex=True)\n",
    "df_mgabund['read_name'] = df_mgabund['read_name'].str.replace(r'.fa', '', regex=True)\n",
    "df_mgabund = df_mgabund[['read_name','average_abund']]\n",
    "\n",
    "# merge\n",
    "df = df_mgabund.merge(df_3mer, on='read_name')\n",
    "\n",
    "# remove any nans\n",
    "df  = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbbe0e-95bd-478e-a6e1-00e360e16969",
   "metadata": {},
   "source": [
    "## Define VAE architecture\n",
    "Latent dimensions: compressed, lower-dimensional representation of the input data learned by the VAE during training.\n",
    "\n",
    "- Set number of latent dimensions for VAE (compress data into a 10-dimensional latent space)\n",
    "- get dimensions of input data (num columns/features)\n",
    "\n",
    "- build the VAE model using input dimensions and latent dimensions. Returns VAE model and encoder part of VAE\n",
    "vae, encoder = build_vae(input_dim, latent_dim): This line calls the build_vae function to create the VAE model. The build_vae function takes two arguments: input_dim (dimensionality of the input data) and latent_dim (number of latent dimensions). It returns two objects: vae, which is the entire VAE model, and encoder, which is a submodel representing the encoder part of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cebd42fc-c248-4434-9d40-1cc4cc15c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of latent dimensions\n",
    "latent_dim = 1\n",
    "\n",
    " # Dimensionality of the input data (numcols)\n",
    "input_dim = scaled_data.shape[1] \n",
    "\n",
    "# build vae model\n",
    "vae, encoder = build_vae(input_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4631ec-ce5f-41ef-a9f1-890241714c14",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "- Standardizes (z-score normalize) data\n",
    "- Use sklearn standardscaler\n",
    "- Used the scaler on input data to transform it into standardized data (by mena and std div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dc5b7ef6-e3c3-483c-a947-6805d507c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data \n",
    "sequence_names, scaled_data = preprocess_data(df.values)\n",
    "preprocessed_df = pd.DataFrame(data=np.concatenate([sequence_names.reshape(-1, 1), scaled_data], axis=1),\n",
    "                                columns=df.columns)\n",
    "# check for nans\n",
    "np.isnan(scaled_data).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f26310a7-37da-407a-ac55-4da24d49b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_name_list = df['read_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7834ec-5e4e-41a5-b219-8df70d959436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "56c6c51b-5901-485f-98a7-4682e31e4eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "953/953 [==============================] - 1s 470us/step - loss: -4547.2446\n",
      "Epoch 2/50\n",
      "953/953 [==============================] - 0s 464us/step - loss: -124656.7812\n",
      "Epoch 3/50\n",
      "953/953 [==============================] - 0s 463us/step - loss: -681287.5625\n",
      "Epoch 4/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -2041267.7500\n",
      "Epoch 5/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -4548010.5000\n",
      "Epoch 6/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -8554868.0000\n",
      "Epoch 7/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -14444168.0000\n",
      "Epoch 8/50\n",
      "953/953 [==============================] - 0s 476us/step - loss: -22592402.0000\n",
      "Epoch 9/50\n",
      "953/953 [==============================] - 0s 479us/step - loss: -33411234.0000\n",
      "Epoch 10/50\n",
      "953/953 [==============================] - 0s 496us/step - loss: -47367816.0000\n",
      "Epoch 11/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -64792668.0000\n",
      "Epoch 12/50\n",
      "953/953 [==============================] - 0s 485us/step - loss: -86247832.0000\n",
      "Epoch 13/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -112348680.0000\n",
      "Epoch 14/50\n",
      "953/953 [==============================] - 0s 465us/step - loss: -143600288.0000\n",
      "Epoch 15/50\n",
      "953/953 [==============================] - 0s 511us/step - loss: -180634400.0000\n",
      "Epoch 16/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -223967920.0000\n",
      "Epoch 17/50\n",
      "953/953 [==============================] - 0s 465us/step - loss: -274156128.0000\n",
      "Epoch 18/50\n",
      "953/953 [==============================] - 0s 465us/step - loss: -331704768.0000\n",
      "Epoch 19/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -397593952.0000\n",
      "Epoch 20/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -472552416.0000\n",
      "Epoch 21/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -557161856.0000\n",
      "Epoch 22/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -652193152.0000\n",
      "Epoch 23/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -758051328.0000\n",
      "Epoch 24/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -876047872.0000\n",
      "Epoch 25/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -1006715008.0000\n",
      "Epoch 26/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -1150881024.0000\n",
      "Epoch 27/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -1309598464.0000\n",
      "Epoch 28/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -1483922816.0000\n",
      "Epoch 29/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -1674375296.0000\n",
      "Epoch 30/50\n",
      "953/953 [==============================] - 0s 463us/step - loss: -1881528320.0000\n",
      "Epoch 31/50\n",
      "953/953 [==============================] - 0s 465us/step - loss: -2107365760.0000\n",
      "Epoch 32/50\n",
      "953/953 [==============================] - 0s 458us/step - loss: -2352008448.0000\n",
      "Epoch 33/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -2617447424.0000\n",
      "Epoch 34/50\n",
      "953/953 [==============================] - 0s 464us/step - loss: -2904182784.0000\n",
      "Epoch 35/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -3213152000.0000\n",
      "Epoch 36/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -3544825088.0000\n",
      "Epoch 37/50\n",
      "953/953 [==============================] - 1s 547us/step - loss: -3900161280.0000\n",
      "Epoch 38/50\n",
      "953/953 [==============================] - 0s 464us/step - loss: -4280703232.0000\n",
      "Epoch 39/50\n",
      "953/953 [==============================] - 0s 463us/step - loss: -4688056320.0000\n",
      "Epoch 40/50\n",
      "953/953 [==============================] - 0s 463us/step - loss: -5124568576.0000\n",
      "Epoch 41/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -5590670336.0000\n",
      "Epoch 42/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -6087232000.0000\n",
      "Epoch 43/50\n",
      "953/953 [==============================] - 0s 461us/step - loss: -6614034432.0000\n",
      "Epoch 44/50\n",
      "953/953 [==============================] - 0s 462us/step - loss: -7174632960.0000\n",
      "Epoch 45/50\n",
      "953/953 [==============================] - 0s 457us/step - loss: -7770743808.0000\n",
      "Epoch 46/50\n",
      "953/953 [==============================] - 0s 459us/step - loss: -8402384896.0000\n",
      "Epoch 47/50\n",
      "953/953 [==============================] - 0s 471us/step - loss: -9068353536.0000\n",
      "Epoch 48/50\n",
      "953/953 [==============================] - 0s 463us/step - loss: -9771800576.0000\n",
      "Epoch 49/50\n",
      "953/953 [==============================] - 0s 460us/step - loss: -10515211264.0000\n",
      "Epoch 50/50\n",
      "953/953 [==============================] - 0s 459us/step - loss: -11299938304.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ccff6e10>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.7 # Adjust this value as needed\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "input_dim = scaled_data.shape[1]\n",
    "vae, encoder = build_vae(input_dim, latent_dim)\n",
    "vae.compile(optimizer='adam', loss='binary_crossentropy')  # Use default loss function\n",
    "vae.fit(scaled_data, scaled_data, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dc947636-9914-48ef-bd8b-4207cd2f8497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953/953 [==============================] - 0s 270us/step\n"
     ]
    }
   ],
   "source": [
    "# Encode the data to the mean of their latent distributions\n",
    "latent_representations = encoder.predict(scaled_data)\n",
    "\n",
    "# Cluster the latent representations\n",
    "clusters, cluster_medoids = online_medoid_clustering(latent_representations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1c0cb37c-f1f0-4413-9798-b758ba118524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "40529913-8320-499a-923c-e01d9e44fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_medoids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5c3a27ac-cf71-481b-81c7-1d76b9e92d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the sequence names in each cluster\n",
    "cluster_sequences = {i: [] for i in range(len(clusters))}\n",
    "\n",
    "# Iterate over each cluster and find the indices of sequences in that cluster\n",
    "for cluster_idx, cluster in enumerate(clusters):\n",
    "    for i, latent_representation in enumerate(latent_representations):\n",
    "        if latent_representation in cluster:\n",
    "            # Append the sequence name to the corresponding cluster\n",
    "            cluster_sequences[cluster_idx].append(sequence_name_list[i])\n",
    "\n",
    "# Print the sequence names in each cluster\n",
    "for cluster_idx, sequences in cluster_sequences.items():\n",
    "    print(f\"Cluster {cluster_idx}: {sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5801e-8d21-4db1-bfcd-622ceccf2d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5eeeb141-79c1-4481-ad96-a04bc1496d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE architecture\n",
    "def build_vae(input_dim, latent_dim):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    h = Dense(32, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    # Sampling layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    \n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_h = Dense(32, activation='relu')\n",
    "    decoder_out = Dense(input_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    outputs = decoder_out(h_decoded)\n",
    "\n",
    "    vae = Model(inputs, outputs)\n",
    "    encoder = Model(inputs, z_mean)\n",
    "    \n",
    "    return vae, encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f4bc929d-ef6c-4169-8878-731229426876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Separate the first column (sequence names)\n",
    "    sequence_names = data[:, 0]\n",
    "    # Exclude the first column (sequence names) from scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data[:, 1:])  # Exclude the first column\n",
    "    return sequence_names, scaled_data\n",
    "\n",
    "# Function for online iterative medoid clustering\n",
    "def online_medoid_clustering(latent_representations, threshold=0.5):\n",
    "    clusters = []\n",
    "    cluster_medoids = []\n",
    "    \n",
    "    for point in latent_representations:\n",
    "        assigned = False\n",
    "        for i, medoid in enumerate(cluster_medoids):\n",
    "            if cosine_similarity([point], [medoid])[0][0] > threshold:\n",
    "                clusters[i].append(point)\n",
    "                assigned = True\n",
    "                break\n",
    "        \n",
    "        if not assigned:\n",
    "            clusters.append([point])\n",
    "            cluster_medoids.append(point)\n",
    "    \n",
    "    return clusters, cluster_medoids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c31cb-1f8f-410d-b228-a28fa44396e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
